{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN Class\n",
    "\n",
    "We are going to build a class for doing HDBSCAN. We want the class to relatively closely mirror sklearn's interface for clustering classes. We will also need to build up a set of useful background tools.\n",
    "\n",
    "## HDBSCAN\n",
    "\n",
    "We can break the HDBSCAN algorithm down into a series of steps. \n",
    "\n",
    "### Mutual reachability graph\n",
    "\n",
    "The first step is the creation of the 'mutual reachability graph' -- this modifies the distance matrix to include a notion of density based on a parameter `min_points`, such that points have a minimal distance to other points that is at least the distance required to have `min_points` other points within that minimal distance:\n",
    "\n",
    "Build the mutual reachability graph from a distance matrix. We are operating with the following definitions per 'Campello, Moulavi, Sander':\n",
    "\n",
    "**Core Distance**: The core distance of an object $x_p \\in X$ with respect to $m_{\\textrm{pts}}$ is the distance from $x_p$ to its $m_{\\textrm{pts}}$-nearest neighbour.\n",
    "\n",
    "**Mutual Reachability Distance**: The mutual reachability distance between two objects $x_p$ and $x_q$ in $X$ with respect to $m_{\\textrm{pts}}$ is defined as $d_{\\textrm{mreach}} = \\max\\{d_{\\textrm{core}}(x_p), d_{\\textrm{core}}(x_q), d(x_p, x_q)\\}$.\n",
    "\n",
    "### Single linkage clustering\n",
    "\n",
    "The second step is to apply single linkage clustering to the mutual reachability graph. We can either implement out own single linkage clustering (so as to support nonsymmetric dissimilarity matrices) or use `fastcluster`. For now the implementation uses `fastcluster` -- the only catch is that we need to convert the mutuual reachability matrix into the single array of upper triangular entries that `scipy.spatial.distance.pdist` outputs.\n",
    "\n",
    "### Relabelling the tree\n",
    "\n",
    "The third step is to relabel and condense the cluster tree based on a parameter `min_cluster_size`. Any branch that contains less than `min_cluster_size` points is deemed to be points \"falling out of an existing cluster\" rather than the splitting of a cluster in two. We therefore label the larger child cluster of the splitting with the parent's id and esnure we record at what $\\lambda = \\frac{1}{\\textrm{distance}}$ values the other points \"fell out\" of the cluster. As we do this throughout the whole tree the number of clusters reduces dramatically. From this relabelled tree we can then generate a table\n",
    "\n",
    "| Parent id | Child id |  Lambda | Child size |\n",
    "|----------:|---------:|--------:|-----------:|\n",
    "|          0|         1|   0.1432|          50|\n",
    "|        ...|       ...|      ...|         ...|\n",
    "\n",
    "by flattening the resulting tree out to be in the same format provided by `scipy.cluster.hierarchy` and `fastcluster`. This will then be useful in the next stage when we compute the \"stability\" of clusters.\n",
    "\n",
    "### Cluster stability computation\n",
    "\n",
    "We assign \"stability\" values to each of the clusters in the relabelled/condensed tree (note that this is significantly fewer then the original single linkage tree. This stability value will then be used to determine which clusters will be selected -- essentially finding a (potentially varying) cut through the cluster hierarchy. We define the stability of the $i^{\\textrm{th}}$ cluster $S(C_i)$ as follows:\n",
    "\n",
    "$S(C_i) = \\Sigma_{x_j \\in C_i}{(\\lambda_{max}(x_j, C_i) - \\lambda_{min}(C_i)) }$ <BR>\n",
    "$= \\Sigma_{x_j \\in C_i}{(1/{\\epsilon_{min}(x_j,C_i)} - 1/{\\epsilon_{max}(C_i)})}$\n",
    "\n",
    "where\n",
    "\n",
    "* $C_i$ is the $ith$ cluster\n",
    "* $x_j \\in C_i$ represents the point contained in cluster $C_i$\n",
    "* $\\lambda = 1/\\epsilon$ is the density threshold.  Increase density threshold decreases cluster sizes.\n",
    "* $\\lambda_{max}(x_j, C_i)$ is the density level beyond which x_j no longer belongs to cluster $C_i$\n",
    "* $\\lambda_{min}(C_i)$ is the density level at which cluster $C_i$ first appears\n",
    "\n",
    "### Selecting stable clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing a slew of useful libraries which we will be making use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastcluster as fc\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as data\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For condensing tree it is useful to have a tree node class that has the relevant properties:\n",
    "* An id (not unique per node since we're doing relabelling)\n",
    "* The distance at which the child nodes split off from their parent.\n",
    "* A list of children -- we don't require a binary tree as we may have many individual leaves splitting off at the same distance value.\n",
    "* A boolean denoting whether the node is a leaf node or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CondensedTreeNode:    \n",
    "    def __init__(self, id, dist, children, size, is_leaf):\n",
    "        self.id = id\n",
    "        self.dist = dist\n",
    "        self.children = children\n",
    "        self.child_size = size\n",
    "        self.is_leaf = is_leaf\n",
    "    \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Node object at %s>' % (\n",
    "            hex(id(self))\n",
    "            )\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ID: %d, Lambda %d, Number of Children %d, \" \\\n",
    "               \"Number of children %d, Leaf node: %s\" % (self.id, self.dist, len(self.children), \n",
    "                                                         self.child_size, self.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For condensing the tree we need to recurse through the tree. At each stage we check if either the right or left branch of the original tree is smaller than `min_cluster_size`. If it is smaller then we need to add all the leaves as children that split off at the given distance. Furthermore if either the right or the left is smaller than `min_cluster_size` then we want to retain the id of the node and pass it on to the larger of the two children (since we have \"points falling out of the cluster\" rather than a split forming new clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_leaves(tree):\n",
    "    \"\"\"Consume a tree object and return a list of leaf nodes\"\"\"\n",
    "    return tree.pre_order(lambda x: x)\n",
    "\n",
    "def condense_tree(tree, min_cluster_size=10, next_id=0):\n",
    "        \n",
    "    #Verbose assert\n",
    "    if tree.count == 0:\n",
    "        print(\"Invalid input: Null tree\")\n",
    "        result = Node(-1, -1, [], -1, False)\n",
    "        return result\n",
    "    elif tree.count == 1:\n",
    "        #Passed in a single node. only that node\n",
    "        result = Node(-1, tree.dist, [], 1, True)\n",
    "        return result\n",
    "        \n",
    "    result = CondensedTreeNode(next_id, tree.dist, [], tree.left.count + tree.right.count, 0)\n",
    "        \n",
    "    #If the left node is too small, add a leaf\n",
    "    if tree.left.count <= min_cluster_size:\n",
    "        leaves = get_leaves(tree.left)\n",
    "        for leaf in leaves:\n",
    "            result.add_child(CondensedTreeNode(-(leaf.id + 1), tree.left.dist, [], 1, True))\n",
    "    elif tree.right.count <= min_cluster_size:\n",
    "        child, next_id = condense_tree(tree.left, min_cluster_size, next_id)\n",
    "        result.add_child(child)\n",
    "    else:\n",
    "        child, next_id = condense_tree(tree.left, min_cluster_size, next_id + 1)\n",
    "        result.add_child(child)\n",
    "            \n",
    "    #If the right node is too small, add a leaf\n",
    "    if tree.right.count <= min_cluster_size:\n",
    "        leaves = get_leaves(tree.right)\n",
    "        for leaf in leaves:\n",
    "            result.add_child(CondensedTreeNode(-(leaf.id + 1), tree.right.dist, [], 1, True))\n",
    "    elif tree.left.count <= min_cluster_size:\n",
    "        child, next_id = condense_tree(tree.right, min_cluster_size, next_id)\n",
    "        result.add_child(child)        \n",
    "    else:\n",
    "        child, next_id = condense_tree(tree.right, min_cluster_size, next_id + 1)\n",
    "        result.add_child(child)\n",
    "        \n",
    "    return result, next_id\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can flatten the tree with another recursive function. Since we need to produce a dataframe at the end we will need an outer function and an inner recursive function. A short function generating all the child information for each node can then be called within the recursive function, and we extend the flattened result with the new child information, and pass the result back to the caller (which extends with this information and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_node(tree_node):\n",
    "    return [(tree_node.id, x.id, 1.0/tree_node.dist, x.child_size) for x in tree_node.children\n",
    "             if tree_node.id != x.id]\n",
    "\n",
    "def flatten_tree_recursion(tree):\n",
    "    if tree.is_leaf:\n",
    "        return []\n",
    "    result = flatten_node(tree)\n",
    "    for subtree in tree.children:\n",
    "        result.extend(flatten_tree_recursion(subtree))\n",
    "    return result\n",
    "\n",
    "def flatten_tree(tree):\n",
    "    result = flatten_tree_recursion(tree)\n",
    "    return pd.DataFrame(result, columns=(\"parent\",\"child\",\"lambda\",\"child_size\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stability of a cluster is $\\sum (\\lambda_{\\textrm{death}} - \\lambda_{\\textrm{birth}})$. We can compute this for each row of the dataframe output by flatten tree if we have $\\lambda_{\\textrm{birth}}$ values for each row. To generate that we need to find the minimum $\\lambda$ value seen (so we group by child id and take the min of the `lambda` column per group). We can then join that to the main dataframe, and compute a stability value for each row. The total stability for each cluster is then the group by parent id sum of stability values. We can then normalise that by the size of the cluster (so as not to bias large clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stability(row):\n",
    "    return (row[\"lambda_death\"] - row[\"lambda_birth\"]) * row[\"child_size\"]\n",
    "\n",
    "def compute_stability(cluster_tree):\n",
    "    births = cluster_tree.groupby(\"child\").min()[[\"lambda\"]]\n",
    "    births_and_deaths = cluster_tree.join(births, on=\"parent\", lsuffix=\"_death\", rsuffix=\"_birth\")\n",
    "    births_and_deaths[\"stability\"] = births_and_deaths.apply(stability, axis=1)\n",
    "    return births_and_deaths.groupby(\"parent\")[[\"stability\"]].sum() / \\\n",
    "            pd.DataFrame(births_and_deaths.parent.value_counts(), columns=[\"stability\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stability_score(tree_node, stability_table):\n",
    "    node_stability = stability_table.loc[tree_node.id][0]\n",
    "    child_stability = sum(max(stability_score(x, stability_table)) for x in tree_node.children if not x.is_leaf)\n",
    "    return (node_stability, child_stability)\n",
    "\n",
    "def get_clusters(tree_node, stability_table, results={}):\n",
    "    tree_node.score = stability_score(tree_node, stability_table)\n",
    "    if tree_node.score[0] > tree_node.score[1]:\n",
    "        tree_node.is_cluster = True\n",
    "        cluster_id = max(results.keys()) + 1 if results.keys() else 0 \n",
    "        results[cluster_id] = tree_node.points\n",
    "    else:\n",
    "        tree_node.is_cluster = False\n",
    "    if not tree_node.is_cluster:\n",
    "        for node in tree_node.children:\n",
    "            if not node.is_leaf:\n",
    "                get_clusters(node, stability_table, results)\n",
    "    return results\n",
    "\n",
    "def get_leaf_point_ids(tree):\n",
    "    results = []\n",
    "    for node in tree.children:\n",
    "        if node.is_leaf:\n",
    "            results.append(-(node.id - 1))\n",
    "        else:\n",
    "            results.extend(get_leaf_point_ids(node))\n",
    "    return results\n",
    "\n",
    "def reduce_tree(tree):\n",
    "    result = CondensedTreeNode(tree.id, 0, [], 0, 0)\n",
    "    result.points = get_leaf_point_ids(tree)\n",
    "    \n",
    "    children_to_process = tree.children[:]\n",
    "    \n",
    "    for child in children_to_process:\n",
    "        if child.is_leaf:\n",
    "            continue\n",
    "        if child.id == tree.id:\n",
    "            children_to_process.extend(child.children)\n",
    "        else:\n",
    "            result.children.append(reduce_tree(child))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `fastcluster` assumes that any multidimensional array passed to it is an $N\\times p$ feature space and proceeds to pass it to a distance calculator we need to reduce our mutual-reachability matrix to something `fastcluster` will recognise as a distance matrix. In this case that is the raw output of `scipy.spatial.distance.pdist` which is a single-dimensional array that contains the upper triangular part of a distance matrix (that is therefore assumed to be symmetric). This is simple enough to generate from a full distance matrix, so we'll provide a utility function to get that done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_matrix_to_pdist(matrix):\n",
    "    result = []\n",
    "    for index, row in enumerate(distance_matrix):\n",
    "        result.append(row[index:])\n",
    "    return np.hstack(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the main class. The is straightforward: we set `min_cluster_size` and `min_points` (and setting `min_points` to `min_cluster_size` if it was not provided) and the fit method simply takes a distance matrix and performs the various steps described above:\n",
    "\n",
    "1. Compute the mutual reachability graph\n",
    "2. Perform single linkage on the resulting mutual reachability graph\n",
    "3. Condense and relabel the resulting tree according to `min_cluster_size`\n",
    "4. Compute the cluster stability and extract the most stable clustering that covers the points best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HDBSCAN (object):\n",
    "    \n",
    "    def __init__(self, min_cluster_size=5, min_points=None):\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        if min_points is None:\n",
    "            self.min_points = min_cluster_size\n",
    "        else:\n",
    "            self.min_points = min_points\n",
    "        self._mutual_reachability_graph = None\n",
    "        self._raw_tree = None\n",
    "        self._tree_frame = None\n",
    "        \n",
    "    def _mutual_reachability(self, distance_matrix):\n",
    "        dim = distance_matrix.shape[0]\n",
    "        core_distances = np.partition(distance_matrix, self.min_points, axis=0)[self.min_points]\n",
    "        stage1 = np.where(core_distances > distance_matrix, core_distances, distance_matrix)\n",
    "        self._mutual_reachability_graph = np.where(core_distances > stage1.T, core_distances.T, stage1.T).T\n",
    "        return\n",
    "    \n",
    "    def _single_linkage(self):\n",
    "        assert(self._mutual_reachability_graph is not None)\n",
    "        #self._raw_tree = single_linkage(self._mutual_reachability_graph)\n",
    "        pdist_array = distance_matrix_to_pdist(self._mutual_reachability_graph)\n",
    "        self._raw_tree = fc.single(pdist_array)\n",
    "        return\n",
    "    \n",
    "    def _condense_tree(self):\n",
    "        assert(self._raw_tree is not None)\n",
    "        base_tree = sch.to_tree(self._raw_tree)\n",
    "        self._condensed_tree, final_id = condense_tree(base_tree, self.min_cluster_size)\n",
    "        self._tree_frame = flatten_tree(self._condensed_tree)\n",
    "        \n",
    "    def _compute_stable_clusters(self):\n",
    "        assert(self._tree_frame is not None)\n",
    "        self._stability_table = compute_stability(self._tree_frame)\n",
    "        self._reduced_tree = reduce_tree(self._condensed_tree)\n",
    "        self._cluster_dict = get_clusters(self._reduced_tree, self._stability_table)\n",
    "    \n",
    "    def fit(self, distance_matrix):\n",
    "        self._mutual_reachability(distance_matrix)\n",
    "        self._single_linkage()\n",
    "        self._condense_tree()\n",
    "        self._compute_stable_clusters()\n",
    "        return self._cluster_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do some quick benchmarking to get an idea of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as dist\n",
    "\n",
    "iris = pd.read_csv(\"iris.csv\")\n",
    "distance_matrix = dist.squareform(dist.pdist(iris.ix[:,:4].as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 76.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clusterer = HDBSCAN(10)\n",
    "clusterer.fit(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = data.load_digits()\n",
    "digits_distance_matrix = dist.squareform(dist.pdist(digits.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 777 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clusterer = HDBSCAN(10)\n",
    "clusterer.fit(digits_distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%%prun\n",
    "clusterer = HDBSCAN(10)\n",
    "clusterer.fit(digits_distance_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "         19273 function calls (18228 primitive calls) in 0.781 seconds\n",
    "\n",
    "   Ordered by: internal time\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.249    0.249    0.249    0.249 {method 'partition' of 'numpy.ndarray' objects}\n",
    "        2    0.219    0.109    0.219    0.109 {numpy.core.multiarray.where}\n",
    "        1    0.136    0.136    0.670    0.670 <ipython-input-6-b903a8b70d4c>:13(_mutual_reachability)\n",
    "        7    0.066    0.009    0.066    0.009 {method 'copy' of 'numpy.ndarray' objects}\n",
    "        1    0.004    0.004    0.778    0.778 <ipython-input-6-b903a8b70d4c>:39(fit)\n",
    "    141/1    0.004    0.000    0.009    0.009 <ipython-input-19-bdba24114173>:5(condense_tree)\n",
    "      455    0.004    0.000    0.025    0.000 series.py:512(__getitem__)\n",
    "      946    0.004    0.000    0.013    0.000 {pandas.lib.values_from_object}\n",
    "      456    0.003    0.000    0.008    0.000 internals.py:3463(get_values)\n",
    "      455    0.003    0.000    0.019    0.000 index.py:1447(get_value)\n",
    "        1    0.003    0.003    0.036    0.036 {pandas.lib.reduce}\n",
    "  726/725    0.003    0.000    0.003    0.000 {numpy.core.multiarray.array}\n",
    "        1    0.003    0.003    0.781    0.781 <string>:2(<module>)\n",
    "     1594    0.003    0.000    0.003    0.000 {isinstance}\n",
    "1656/1183    0.003    0.000    0.004    0.000 {len}\n",
    "      455    0.003    0.000    0.003    0.000 {method 'get_value' of 'pandas.index.IndexEngine' objects}\n",
    "        1    0.002    0.002    0.006    0.006 hierarchy.py:838(to_tree)\n",
    "      151    0.002    0.000    0.027    0.000 <ipython-input-4-efc3bd24b6e5>:1(stability)\n",
    "      142    0.002    0.000    0.002    0.000 hierarchy.py:772(pre_order)\n",
    "      163    0.002    0.000    0.006    0.000 series.py:250(_set_axis)\n",
    "       15    0.002    0.000    0.004    0.000 internals.py:2250(_rebuild_blknos_and_blklocs)\n",
    "    292/1    0.002    0.000    0.003    0.003 <ipython-input-3-48ccaec20997>:5(flatten_tree_recursion)\n",
    "      533    0.001    0.000    0.001    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
    "      456    0.001    0.000    0.009    0.000 series.py:312(get_values)\n",
    "      478    0.001    0.000    0.002    0.000 numeric.py:1910(isscalar)\n",
    "      150    0.001    0.000    0.003    0.000 shape_base.py:8(atleast_1d)\n",
    "      301    0.001    0.000    0.001    0.000 hierarchy.py:683(__init__)\n",
    "    141/1    0.001    0.000    0.002    0.002 <ipython-input-4-efc3bd24b6e5>:29(get_leaf_point_ids)\n",
    "        1    0.001    0.001    0.001    0.001 hierarchy.py:1372(_check_hierarchy_uses_cluster_more_than_once)\n",
    "      456    0.001    0.000    0.002    0.000 internals.py:112(to_dense)\n",
    "       22    0.001    0.000    0.004    0.000 index.py:127(__new__)\n",
    "       40    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
    "      151    0.001    0.000    0.003    0.000 internals.py:2204(set_axis)\n",
    "      358    0.001    0.000    0.004    0.000 index.py:4888(_ensure_index)\n",
    "      463    0.001    0.000    0.001    0.000 index.py:256(__len__)\n",
    "      141    0.001    0.000    0.001    0.000 <ipython-input-3-48ccaec20997>:1(flatten_node)\n",
    "      934    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}\n",
    "       30    0.001    0.000    0.001    0.000 {pandas.lib.infer_dtype}\n",
    "      618    0.001    0.000    0.001    0.000 internals.py:3372(_block)\n",
    "        4    0.001    0.000    0.001    0.000 {pandas.lib.maybe_convert_objects}\n",
    "       50    0.001    0.000    0.002    0.000 internals.py:2196(shape)\n",
    "      293    0.001    0.000    0.001    0.000 <ipython-input-2-171f8947c123>:2(__init__)\n",
    "        1    0.001    0.001    0.004    0.004 <ipython-input-5-9e3ac38f0fbc>:1(distance_matrix_to_pdist)\n",
    "      291    0.001    0.000    0.001    0.000 <ipython-input-2-171f8947c123>:9(add_child)\n",
    "      576    0.001    0.000    0.001    0.000 {method 'extend' of 'list' objects}\n",
    "      163    0.001    0.000    0.001    0.000 series.py:270(_set_subtyp)\n",
    "       21    0.001    0.000    0.001    0.000 common.py:2320(_asarray_tuplesafe)\n",
    "       57    0.001    0.000    0.001    0.000 {numpy.core.multiarray.empty}\n",
    "      142    0.001    0.000    0.003    0.000 <ipython-input-19-bdba24114173>:1(get_leaves)\n",
    "      165    0.001    0.000    0.001    0.000 numeric.py:464(asanyarray)\n",
    "       14    0.001    0.000    0.002    0.000 common.py:735(take_nd)\n",
    "       12    0.001    0.000    0.001    0.000 common.py:1947(_possibly_cast_to_datetime)\n",
    "      303    0.001    0.000    0.001    0.000 {issubclass}\n",
    "      ...      ...      ...      ...      ... ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of our time is spent in the mutual reachability computation, and within that more than half our time is spent in the (likely very well optimized) partition function used to find the distance of the `min_points`th nearest neighbour distance. It is unlikely we are going to speed that operation up much without some hand-coded C to try and do the job efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
